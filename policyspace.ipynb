{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "np.random.seed(0)  # for reproducibility\n",
    "num_states = 5\n",
    "num_actions = 30\n",
    "gamma = 0.9\n",
    "\n",
    "# Random transition probabilities P[s, a, s'] — rows must sum to 1\n",
    "P = np.zeros((num_states, num_actions, num_states))\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        probs = np.random.rand(num_states)\n",
    "        probs /= probs.sum()  # normalize to sum to 1\n",
    "        P[s, a] = probs\n",
    "\n",
    "# Random rewards R[s, a] in range [-10, 10]\n",
    "R = np.random.uniform(-10, 10, (num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionsub=[16,14,15,25,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Policy = [ 1 22  3 16  3]\n",
      "Iteration 1: Policy = [14 14 15 25  8]\n",
      "Iteration 2: Policy = [16 14 15 25  8]\n",
      "\n",
      "Optimal Policy: [16 14 15 25  8]\n",
      "Value Function: [91.661 91.558 90.363 90.807 90.22 ]\n"
     ]
    }
   ],
   "source": [
    "# Initialize policy arbitrarily\n",
    "policy = np.random.choice(num_actions, size=num_states)\n",
    "\n",
    "def policy_evaluation(policy, P, R, gamma, tol=1e-6):\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            a = policy[s]\n",
    "            v = V[s]\n",
    "            V[s] = R[s, a] + gamma * np.dot(P[s, a], V)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, P, R, gamma):\n",
    "    policy_stable = True\n",
    "    new_policy = np.zeros(num_states, dtype=int)\n",
    "    for s in range(num_states):\n",
    "        old_action = policy[s]\n",
    "        action_values = [\n",
    "            R[s, a] + gamma * np.dot(P[s, a], V) for a in actionsub\n",
    "        ]\n",
    "        new_policy[s] = actionsub[np.argmax(action_values)]\n",
    "        if new_policy[s] != old_action:\n",
    "            policy_stable = False\n",
    "    return new_policy, policy_stable\n",
    "\n",
    "# Policy Iteration\n",
    "iteration = 0\n",
    "while True:\n",
    "    print(f\"Iteration {iteration}: Policy = {policy}\")\n",
    "    V = policy_evaluation(policy, P, R, gamma)\n",
    "    policy, stable = policy_improvement(V, P, R, gamma)\n",
    "    if stable:\n",
    "        break\n",
    "    iteration += 1\n",
    "\n",
    "# Final results\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"\\nOptimal Policy:\", policy)\n",
    "print(\"Value Function:\", V)\n",
    "# print(\"\\nTransition Probabilities P (shape {}):\".format(P.shape))\n",
    "# print(P)\n",
    "# print(\"\\nReward Matrix R (shape {}):\".format(R.shape))\n",
    "# print(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Policy = [ 8 23 19 15  7]\n",
      "Iteration 1: Policy = [16 14 26 14 19]\n",
      "Iteration 2: Policy = [16 14 15 14  8]\n",
      "Iteration 3: Policy = [16 14 15 25  8]\n",
      "\n",
      "Optimal Policy: [16 14 15 25  8]\n",
      "Value Function: [91.661 91.558 90.363 90.807 90.22 ]\n"
     ]
    }
   ],
   "source": [
    "# Initialize policy arbitrarily\n",
    "policy = np.random.choice(num_actions, size=num_states)\n",
    "\n",
    "def policy_evaluation(policy, P, R, gamma, tol=1e-6):\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            a = policy[s]\n",
    "            v = V[s]\n",
    "            V[s] = R[s, a] + gamma * np.dot(P[s, a], V)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, P, R, gamma):\n",
    "    policy_stable = True\n",
    "    new_policy = np.zeros(num_states, dtype=int)\n",
    "    for s in range(num_states):\n",
    "        old_action = policy[s]\n",
    "        action_values = [\n",
    "            R[s, a] + gamma * np.dot(P[s, a], V) for a in range(num_actions)\n",
    "        ]\n",
    "        new_policy[s] = np.argmax(action_values)\n",
    "        if new_policy[s] != old_action:\n",
    "            policy_stable = False\n",
    "    return new_policy, policy_stable\n",
    "\n",
    "# Policy Iteration\n",
    "iteration = 0\n",
    "while True:\n",
    "    print(f\"Iteration {iteration}: Policy = {policy}\")\n",
    "    V = policy_evaluation(policy, P, R, gamma)\n",
    "    policy, stable = policy_improvement(V, P, R, gamma)\n",
    "    if stable:\n",
    "        break\n",
    "    iteration += 1\n",
    "\n",
    "# Final results\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"\\nOptimal Policy:\", policy)\n",
    "print(\"Value Function:\", V)\n",
    "# print(\"\\nTransition Probabilities P (shape {}):\".format(P.shape))\n",
    "# print(P)\n",
    "# print(\"\\nReward Matrix R (shape {}):\".format(R.shape))\n",
    "# print(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_deterministic_policies(n_states, n_actions):\n",
    "    \"\"\"Generate all deterministic policies as one-hot matrices.\"\"\"\n",
    "    all_policies = []\n",
    "    for actions in product(range(n_actions), repeat=n_states):\n",
    "        policy = np.zeros((n_states, n_actions))\n",
    "        for s, a in enumerate(actions):\n",
    "            policy[s, a] = 1\n",
    "        all_policies.append(policy)\n",
    "    return all_policies\n",
    "\n",
    "def decompose_random_policy(pi, deterministic_policies, seed=None):\n",
    "    \"\"\"\n",
    "    Decompose a random policy into a convex combination of deterministic policies.\n",
    "    You can randomize the LP objective to find different decompositions.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_states, n_actions = pi.shape\n",
    "    N = len(deterministic_policies)\n",
    "    \n",
    "    # Flatten each deterministic policy\n",
    "    P = np.array([policy.flatten() for policy in deterministic_policies]).T  # shape: (n_states * n_actions, N)\n",
    "    b = pi.flatten()\n",
    "\n",
    "    # Add equality constraints: sum_i λ_i P_i = pi, and sum_i λ_i = 1\n",
    "    A_eq = np.vstack([P, np.ones(N)])\n",
    "    b_eq = np.concatenate([b, [1]])\n",
    "\n",
    "    # Use a random objective to get a different decomposition\n",
    "    c = np.random.rand(N)\n",
    "\n",
    "    res = linprog(c=c, A_eq=A_eq, b_eq=b_eq, bounds=[(0, 1)] * N, method=\"highs\")\n",
    "\n",
    "    if res.success:\n",
    "        weights = res.x\n",
    "        decomposition = [(w, deterministic_policies[i]) for i, w in enumerate(weights) if w > 1e-6]\n",
    "        return decomposition\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def print_decomposition(decomposition):\n",
    "    for i, (weight, policy) in enumerate(decomposition):\n",
    "        print(f\"\\nPolicy {i + 1}: weight = {weight:.4f}\")\n",
    "        print(policy.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target randomized policy:\n",
      "[[0.6 0.4]\n",
      " [0.2 0.8]\n",
      " [0.5 0.5]]\n",
      "\n",
      "First decomposition:\n",
      "\n",
      "Policy 1: weight = 0.1000\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "\n",
      "Policy 2: weight = 0.5000\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "\n",
      "Policy 3: weight = 0.1000\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "\n",
      "Policy 4: weight = 0.3000\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "\n",
      "Second (different) decomposition:\n",
      "\n",
      "Policy 1: weight = 0.1500\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "\n",
      "Policy 2: weight = 0.4500\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "\n",
      "Policy 3: weight = 0.0500\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "\n",
      "Policy 4: weight = 0.3500\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "n_states = 3\n",
    "n_actions = 2\n",
    "\n",
    "# Randomized policy: each row is a probability distribution over actions for a state\n",
    "pi = np.array([\n",
    "    [0.6, 0.4],\n",
    "    [0.2, 0.8],\n",
    "    [0.5, 0.5]\n",
    "])\n",
    "\n",
    "print(\"Target randomized policy:\")\n",
    "print(pi)\n",
    "\n",
    "# Generate all deterministic policies\n",
    "det_policies = generate_all_deterministic_policies(n_states, n_actions)\n",
    "\n",
    "# Get two different decompositions using different seeds\n",
    "print(\"\\nFirst decomposition:\")\n",
    "decomp1 = decompose_random_policy(pi, det_policies, seed=0)\n",
    "print_decomposition(decomp1)\n",
    "\n",
    "print(\"\\nSecond (different) decomposition:\")\n",
    "decomp2 = decompose_random_policy(pi, det_policies, seed=42)\n",
    "print_decomposition(decomp2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
