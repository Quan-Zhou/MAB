{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2030cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from Qtabularfunctions import*\n",
    "from Cartpolefamily import*\n",
    "from metaQlearning import*\n",
    "from plotfunctions import*\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21b92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state space limits\n",
    "low = np.array([-4.8, -3.0, -0.418, -3.5])\n",
    "high = np.array([4.8, 3.0, 0.418, 3.5])\n",
    "\n",
    "min_td_error = 1e-3  # Minimum TD error threshold to continue episode\n",
    "consecutive_small_errors = 5  # Number of consecutive small TD errors to trigger stop\n",
    "\n",
    "num_actions = 501\n",
    "lr=0.1\n",
    "gamma=0.99\n",
    "epsilon=1\n",
    "force_mag=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73752c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = CartPoleCategoryGenerator()\n",
    "\n",
    "#initial:\n",
    "agent = TabularQLearningAgent(\n",
    "    statespace=[low,high],\n",
    "    num_actions=num_actions,  \n",
    "    lr=lr,\n",
    "    gamma=gamma,\n",
    "    epsilon=epsilon,\n",
    "    force_mag=force_mag\n",
    ")\n",
    "\n",
    "actionspace_dict = {state_tuple: [*range(num_actions)] for state_tuple in agent.disc.get_all_discrete_states()}\n",
    "actionset_dict = {state_tuple: [] for state_tuple in agent.disc.get_all_discrete_states()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1326c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = MetaQLearningRunner(\n",
    "    gen=gen,  # your environment generator\n",
    "    low=low,\n",
    "    high=high, \n",
    "    num_actions=num_actions,\n",
    "    actionset_dict=actionset_dict,\n",
    "    actionspace_dict=actionspace_dict,\n",
    "    lr=lr,\n",
    "    gamma=gamma,\n",
    "    epsilon=epsilon,\n",
    "    force_mag=force_mag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84782477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 10 experiments with:\n",
      "  Episodes per run: 1000\n",
      "  Using actionset as action space: False\n",
      "  Updating actionset_dict: True\n",
      "\n",
      "--- Starting Run 1/10 ---\n",
      "Starting experiment with category: medium\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed in 1.88 seconds\n",
      "Mean reward: 20.93 ± 9.23\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 1 completed in 1.88s\n",
      "\n",
      "--- Starting Run 2/10 ---\n",
      "Starting experiment with category: medium\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.93 seconds\n",
      "Mean reward: 21.77 ± 7.97\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 2 completed in 1.93s\n",
      "\n",
      "--- Starting Run 3/10 ---\n",
      "Starting experiment with category: easy\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.38 seconds\n",
      "Mean reward: 17.08 ± 5.99\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 3 completed in 1.38s\n",
      "\n",
      "--- Starting Run 4/10 ---\n",
      "Starting experiment with category: medium\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 2.10 seconds\n",
      "Mean reward: 24.92 ± 11.29\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 4 completed in 2.10s\n",
      "\n",
      "--- Starting Run 5/10 ---\n",
      "Starting experiment with category: easy\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.70 seconds\n",
      "Mean reward: 19.14 ± 9.61\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 5 completed in 1.70s\n",
      "\n",
      "--- Starting Run 6/10 ---\n",
      "Starting experiment with category: medium\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.78 seconds\n",
      "Mean reward: 21.13 ± 9.56\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 6 completed in 1.78s\n",
      "\n",
      "--- Starting Run 7/10 ---\n",
      "Starting experiment with category: easy\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.64 seconds\n",
      "Mean reward: 17.56 ± 7.83\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 7 completed in 1.64s\n",
      "\n",
      "--- Starting Run 8/10 ---\n",
      "Starting experiment with category: easy\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.39 seconds\n",
      "Mean reward: 15.85 ± 5.99\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 8 completed in 1.39s\n",
      "\n",
      "--- Starting Run 9/10 ---\n",
      "Starting experiment with category: medium\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 2.27 seconds\n",
      "Mean reward: 27.05 ± 15.73\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 9 completed in 2.27s\n",
      "\n",
      "--- Starting Run 10/10 ---\n",
      "Starting experiment with category: easy\n",
      "Using actionspace_dict as action space (training (updating actionset_dict))\n",
      "Experiment completed in 1.64 seconds\n",
      "Mean reward: 19.31 ± 9.33\n",
      "Early stops: 0/1000\n",
      "Mode: training\n",
      "Actionset dictionary was updated\n",
      "Run 10 completed in 1.64s\n",
      "\n",
      "--- Multiple Runs Summary ---\n",
      "Total runtime: 17.71s\n",
      "Average runtime per run: 1.77 ± 0.27s\n",
      "Average reward: 20.48 ± 3.31\n",
      "actionset_dict saved to trained_actionset.pkl\n"
     ]
    }
   ],
   "source": [
    "num_runs=10\n",
    "\n",
    "# Train using actionspace_dict and build actionset_dict\n",
    "runner.run_multiple_experiments(\n",
    "    num_runs=num_runs, \n",
    "    episodes_per_run=1000,\n",
    "    update_actionset_dict=True,\n",
    "    use_actionset_as_actionspace=False,  # Training mode\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Remove repetitions\n",
    "runner.remove_action_repetitions()\n",
    "\n",
    "# Save the trained actionset_dict for later use\n",
    "runner.save_actionset_dict('trained_actionset.pkl')\n",
    "\n",
    "# Evaluate both action sets for 30 times\n",
    "print(\"\\n=== Phase 2: Comparing Policy Performance ===\")\n",
    "comparison_results = runner.compare_policy_performance(\n",
    "    training_episodes=1000,\n",
    "    evaluation_episodes=1,\n",
    "    num_comparisons=30\n",
    ")\n",
    "\n",
    "# Step 4: Individual detailed evaluations\n",
    "print(\"\\n=== Phase 3: Detailed Individual Evaluations ===\")\n",
    "plot_training_comparison(comparison_results['detailed_results'])\n",
    "plot_runtime_vs_reward(comparison_results['detailed_results'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
